{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fall2_Cam4.mp4\n",
      "2870 2920 2910 3000\n",
      "2880 2930 2910 3000\n",
      "2890 2940 2910 3000\n",
      "2900 2950 2910 3000\n",
      "2910 2960 2910 3000\n",
      "2920 2970 2910 3000\n",
      "2930 2980 2910 3000\n",
      "2940 2990 2910 3000\n",
      "2950 3000 2910 3000\n",
      "2960 3010 2910 3000\n",
      "2970 3020 2910 3000\n",
      "2980 3030 2910 3000\n",
      "2990 3040 2910 3000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "originals_dir = \"originals\"\n",
    "output_dir = \"MMACTION\"\n",
    "frame_length = 50  # Number of frames in each clip\n",
    "sliding_window = 10  # Sliding window step in frames\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_subdir = os.path.join(output_dir, str(frame_length))\n",
    "os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "# Load the labels from labels.txt\n",
    "labels_df = pd.read_csv('labels.txt', sep=\"\\t\")\n",
    "\n",
    "# Open the annotation file for writing\n",
    "annotation_file_path = os.path.join(output_dir, f\"{frame_length}.txt\")\n",
    "with open(annotation_file_path, 'w') as annotation_file:\n",
    "\n",
    "    # Process each video file in the originals directory\n",
    "    for video_file in os.listdir(originals_dir):\n",
    "        if video_file.startswith(\"Fall\") and video_file.endswith(\".mp4\"):\n",
    "            print(f\"Processing {video_file}\")\n",
    "            parts = video_file.replace(\".mp4\", \"\").split(\"_\")\n",
    "            clip_no = int(parts[0].replace(\"Fall\", \"\"))\n",
    "            cam_no = parts[1]\n",
    "\n",
    "            # Get the fall start and end times for this clip\n",
    "            fall_info = labels_df[labels_df['Clip'] == clip_no]\n",
    "            fall_start = fall_info['Start'].values[0]  # In seconds\n",
    "            fall_end = fall_info['End'].values[0]  # In seconds\n",
    "\n",
    "            # Open the video file\n",
    "            video_path = os.path.join(originals_dir, video_file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "            # Convert fall start and end times to frames\n",
    "            fall_start_frame = int(fall_start * fps)\n",
    "            fall_end_frame = int(fall_end * fps)\n",
    "\n",
    "            fall_clips = []\n",
    "            non_fall_clips = []\n",
    "\n",
    "            # --- Generate fall clips ---\n",
    "            # Generate clips strictly within the fall window\n",
    "            for start_frame in range(fall_start_frame-frame_length+sliding_window, fall_end_frame, sliding_window):\n",
    "                end_frame = start_frame + frame_length\n",
    "\n",
    "                # Ensure the clip contains part of the fall\n",
    "                is_fall = not (end_frame < fall_start_frame or start_frame > fall_end_frame)\n",
    "                fall_label = 1 if is_fall else 0\n",
    "\n",
    "                # Output filename\n",
    "                output_name = f\"{clip_no}_{fall_label}_{cam_no}_fall_{start_frame}_{end_frame}.mp4\"\n",
    "                output_path = os.path.join(output_subdir, output_name)\n",
    "\n",
    "                # Extract frames\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                frames = []\n",
    "                for _ in range(frame_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        # Append black frames if not enough frames left\n",
    "                        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                        frames.append(black_frame)\n",
    "\n",
    "                # Write the frames to a video file\n",
    "                out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "                for frame in frames:\n",
    "                    out.write(frame)\n",
    "                out.release()\n",
    "\n",
    "                # Write the annotation for this clip to the annotation file\n",
    "                annotation_file.write(f\"{output_name} {fall_label}\\n\")\n",
    "\n",
    "                # Append fall clip to list\n",
    "                fall_clips.append(output_name)\n",
    "\n",
    "            # --- Generate non-fall clips ---\n",
    "            # We need to sample non-fall clips from outside the fall range\n",
    "            non_fall_start = 0\n",
    "            non_fall_end = total_frames\n",
    "            valid_non_fall_ranges = []\n",
    "\n",
    "            # Add valid non-fall ranges outside the fall window\n",
    "            if non_fall_start < fall_start_frame - frame_length:\n",
    "                valid_non_fall_ranges.append((non_fall_start, fall_start_frame - frame_length))\n",
    "            if fall_end_frame + frame_length < non_fall_end:\n",
    "                valid_non_fall_ranges.append((fall_end_frame + frame_length, non_fall_end))\n",
    "\n",
    "            # Sample non-fall clips until the number of fall and non-fall clips are equal\n",
    "            while len(non_fall_clips) < len(fall_clips):\n",
    "                # Randomly choose a range to sample from\n",
    "                chosen_range = random.choice(valid_non_fall_ranges)\n",
    "                range_start, range_end = chosen_range\n",
    "\n",
    "                # Randomly sample a starting frame within this valid range\n",
    "                start_frame = random.randint(range_start, range_end - frame_length)\n",
    "                end_frame = start_frame + frame_length\n",
    "\n",
    "                non_fall_label = 0\n",
    "\n",
    "                # Output filename for non-fall clip\n",
    "                output_name = f\"{clip_no}_{non_fall_label}_{cam_no}_nonfall_{start_frame}_{end_frame}.mp4\"\n",
    "                output_path = os.path.join(output_subdir, output_name)\n",
    "\n",
    "                # Extract frames\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                frames = []\n",
    "                for _ in range(frame_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        # Append black frames if not enough frames left\n",
    "                        black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                        frames.append(black_frame)\n",
    "\n",
    "                # Write the frames to a video file\n",
    "                out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "                for frame in frames:\n",
    "                    out.write(frame)\n",
    "                out.release()\n",
    "\n",
    "                # Write the annotation for this clip to the annotation file\n",
    "                annotation_file.write(f\"{output_name} {non_fall_label}\\n\")\n",
    "\n",
    "                # Append non-fall clip to list\n",
    "                non_fall_clips.append(output_name)\n",
    "\n",
    "            # Release the video capture\n",
    "            cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train clips: 20\n",
      "Number of validation clips: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "split_ratio = 0.8  # 80/20 split for training and validation\n",
    "output_dir = \"MMACTION\"\n",
    "balanced_annotation_file_path = os.path.join(output_dir, f\"{frame_length}.txt\")\n",
    "\n",
    "# Paths to the directories\n",
    "train_dir = os.path.join(output_dir, f\"{frame_length}_train\")\n",
    "val_dir = os.path.join(output_dir, f\"{frame_length}_validation\")\n",
    "\n",
    "# Create train and validation directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Lists to hold the filenames for train and validation splits\n",
    "train_clips = []\n",
    "val_clips = []\n",
    "\n",
    "# Read the balanced annotation file\n",
    "with open(balanced_annotation_file_path, 'r') as f:\n",
    "    clips_with_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Shuffle the clips for randomness\n",
    "random.shuffle(clips_with_labels)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(clips_with_labels) * split_ratio)\n",
    "\n",
    "# Split the clips into train and validation sets\n",
    "train_clips = clips_with_labels[:split_index]\n",
    "val_clips = clips_with_labels[split_index:]\n",
    "\n",
    "# Function to move files and write annotations\n",
    "def move_files_and_create_annotation(clips, folder, annotation_filename):\n",
    "    annotation_file_path = os.path.join(output_dir, annotation_filename)\n",
    "    with open(annotation_file_path, 'w') as annotation_file:\n",
    "        for clip in clips:\n",
    "            filename, label = clip.split()\n",
    "            label = int(label)\n",
    "            \n",
    "            # Move the video file\n",
    "            source_video_path = os.path.join(output_dir, str(frame_length))\n",
    "            source_video_path = os.path.join(source_video_path, filename)\n",
    "            dest_video_path = os.path.join(folder, filename)\n",
    "            shutil.move(source_video_path, dest_video_path)\n",
    "            \n",
    "            # Write the annotation to the corresponding .txt file\n",
    "            annotation_file.write(f\"{filename} {label}\\n\")\n",
    "\n",
    "# Move the files and create the annotation files for train and validation\n",
    "move_files_and_create_annotation(train_clips, train_dir, f\"{frame_length}_train.txt\")\n",
    "move_files_and_create_annotation(val_clips, val_dir, f\"{frame_length}_validation.txt\")\n",
    "\n",
    "# Output the number of clips in each split\n",
    "print(f\"Number of train clips: {len(train_clips)}\")\n",
    "print(f\"Number of validation clips: {len(val_clips)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed folder: MMACTION/50\n",
      "Removed annotation file: MMACTION/50.txt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to the frame_length folder and original annotation file\n",
    "frame_length_folder = os.path.join(output_dir, str(frame_length))\n",
    "original_annotation_file = os.path.join(output_dir, f\"{frame_length}.txt\")\n",
    "\n",
    "# Remove the frame_length folder if it exists\n",
    "if os.path.exists(frame_length_folder):\n",
    "    shutil.rmtree(frame_length_folder)\n",
    "    print(f\"Removed folder: {frame_length_folder}\")\n",
    "else:\n",
    "    print(f\"Folder {frame_length_folder} does not exist.\")\n",
    "\n",
    "# Remove the original annotation file if it exists\n",
    "if os.path.exists(original_annotation_file):\n",
    "    os.remove(original_annotation_file)\n",
    "    print(f\"Removed annotation file: {original_annotation_file}\")\n",
    "else:\n",
    "    print(f\"Annotation file {original_annotation_file} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OFCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
