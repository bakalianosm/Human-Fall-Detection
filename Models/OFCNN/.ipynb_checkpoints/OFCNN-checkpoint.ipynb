{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1360a014-443f-4e95-acbe-80d2991484df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#https://github.com/AdrianNunez/Fall-Detection-with-CNNs-and-Optical-Flow\n",
    "\n",
    "class FallDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with subdirectories of frames for each clip.\n",
    "            sequence_length (int): Number of frames to stack as channels.\n",
    "            transform (callable, optional): Transform to apply to each frame.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.clips = []  # Each entry: (path_to_clip, label)\n",
    "\n",
    "        # Scan the directory\n",
    "        for clip_folder in os.listdir(root_dir):\n",
    "            label = 1 if \"FALL\" in clip_folder else 0\n",
    "            self.clips.append((os.path.join(root_dir, clip_folder), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_path, label = self.clips[idx]\n",
    "        frames = []\n",
    "\n",
    "        # Load each frame in the directory\n",
    "        for frame_file in sorted(os.listdir(clip_path))[:self.sequence_length]:\n",
    "            frame_path = os.path.join(clip_path, frame_file)\n",
    "            frame = Image.open(frame_path)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "        # Stack frames along the channel dimension\n",
    "        stacked_frames = torch.cat(frames, dim=0)\n",
    "        return stacked_frames, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc996e3e-953a-409d-bcea-cac720ffcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "#Multi Channel VGG with last 2 FC layers unfrozen for training and 0.9, 0.8 dropout applied to last 2 fc layers\n",
    "\n",
    "class VGG16MultiChannel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=1):\n",
    "        super(VGG16MultiChannel, self).__init__()\n",
    "        \n",
    "        # Load the pretrained VGG16 model\n",
    "        self.vgg = models.vgg16(pretrained=True)\n",
    "        \n",
    "        # Modify the input layer to accept the specified number of channels\n",
    "        self.vgg.features[0] = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.vgg.classifier[2] = nn.Dropout(0.9)  # Adjust dropout based on paper\n",
    "        self.vgg.classifier[5] = nn.Dropout(0.8) # Adjust dropout based on paper\n",
    "        # Modify the classifier to output a single class for binary classification\n",
    "        self.vgg.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Freezing all but last 2 layers based on paper\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False  # Freeze all parameters\n",
    "        # Unfreeze the last two fully connected layers for training\n",
    "        for param in self.vgg.classifier[3:].parameters():\n",
    "            param.requires_grad = True  # Unfreeze last 2 layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47a54ee-32fd-4408-8a3a-ba5aa4427e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/OFCNN/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tommy/miniconda3/envs/OFCNN/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.9, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.8, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGG16MultiChannel(input_channels = 1).to(device)\n",
    "#View the model\n",
    "print(model.vgg.features)\n",
    "print(model.vgg.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e8e9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "input_channels = (2*24) # 1 second at 25 fps for x and y flow\n",
    "# Dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = FallDataset(root_dir=\"../../Data/RFDS/OFCNN/1.0\", sequence_length=input_channels, transform=transform)\n",
    "\n",
    "# Calculate the size of the validation set\n",
    "val_split = 0.2\n",
    "val_size = int(len(dataset) * val_split)\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6afdd919-2fbb-46a5-a472-4291c521a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n",
      "tensor([0.5923, 0.4382], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.1259, 0.6472], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.4177, 0.3036], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1.])\n",
      "tensor([0.3790, 0.4568], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 1.])\n",
      "tensor([0.5196, 0.5617], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.3716, 0.3839], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.4022, 0.4727], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 1.])\n",
      "tensor([0.4039, 0.1842], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.5435, 0.4292], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.3416, 0.7083], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.4374, 0.2571], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 0.])\n",
      "tensor([0.5580, 0.4283], grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/2], Train Loss: 0.6641, Val Loss: 0.6167\n",
      "Validation loss improved. Saving model at epoch 1.\n",
      "tensor([0., 0.])\n",
      "tensor([0.5009, 0.1905], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.1332, 0.3513], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.3269, 0.4394], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.3081, 0.2178], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1.])\n",
      "tensor([0.2435, 0.6646], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.1062, 0.3401], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 1.])\n",
      "tensor([0.1156, 0.1711], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.2351, 0.1720], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.1351, 0.2375], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1.])\n",
      "tensor([0.1784, 0.1570], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.1723, 0.2337], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0.])\n",
      "tensor([0.2925, 0.3191], grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/2], Train Loss: 0.5392, Val Loss: 0.6225\n",
      "Validation loss did not improve. 1/100 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.0001 # 10^-4 \n",
    "patience = 100 \n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG16MultiChannel(input_channels = input_channels).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for frames, labels in train_loader:\n",
    "        frames, labels = frames.to(device), labels.to(device).float()\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        outputs = outputs.squeeze()  # Remove extra dimension for binary classification\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(labels)\n",
    "        print(outputs)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training loss\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in val_loader:\n",
    "            frames, labels = frames.to(device), labels.to(device).float()\n",
    "            outputs = model(frames)\n",
    "            outputs = outputs.squeeze()  # Remove extra dimension for binary classification\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Log training and validation progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check if the validation loss improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        best_epoch = epoch\n",
    "        # Save the model weights with the best validation loss\n",
    "        torch.save(model.state_dict(),'best_model.pth')\n",
    "        print(f\"Validation loss improved. Saving model at epoch {epoch+1}.\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"Validation loss did not improve. {epochs_without_improvement}/{patience} epochs without improvement.\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f5f07-18d5-45c0-aec7-264f3f8ab60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
